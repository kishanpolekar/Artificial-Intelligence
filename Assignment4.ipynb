{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU4Sqyki1dC0",
        "colab_type": "code",
        "outputId": "1c53fe18-c92b-42fe-e94a-5ddbc63615c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRBG-93YC4V7",
        "colab_type": "code",
        "outputId": "c5b9ae55-9c11-4117-b13e-9c9dcdfdc09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "DATA_PATH = 'data/imdb_reviews.csv'\n",
        "if not Path(DATA_PATH).is_file():\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',\n",
        "        dest_path=DATA_PATH)\n",
        "    \n",
        "df = pd.read_csv(DATA_PATH)  \n",
        "len(df.label.tolist())\n",
        "data = df.iloc[:,0]\n",
        "labels = df.iloc[:,1].tolist()\n",
        "\n",
        "def split_data(review, label, training_ratio):\n",
        "    total = len(review)\n",
        "    n = round(total * training_ratio)\n",
        "    \n",
        "    training_data = review[0:n]\n",
        "    training_lables = label[0:n]\n",
        "    evaluation_data = review[n:].tolist()\n",
        "    evaluation_labels = label[n:]\n",
        "    \n",
        "    return training_data, training_lables, evaluation_data, evaluation_labels\n",
        "  \n",
        "  \n",
        "train_data, train_labels, test_data, test_labels = split_data(data, labels, 0.7)\n",
        "print(len(train_data), ' ', len(test_data))\n",
        "train_data[0]  \n",
        "test_data[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43508   18647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This gripping tale of intergenerational love, jealousy and revenge was even more enjoyable to see on DVD years after its PBS broadcast, with a sharper picture and crisper sound. My only reservations are that the plot has a few improbable moments and that some of the stronger Manchester accents are difficult at times. Luckily even missing a word here and there won\\'t spoil the fun: the primary actors are ideally cast. Robson Green brings an enigmatic smile, a go-for-broke temperament and an athletic physicality to his role as a young surgeon who falls hopelessly for the wife of his boss at the hospital where he\\'s just begun to work. Francesca Annis is one of the most striking 50-ish women imaginable; her acting rivals her beauty. (The love scenes between these two demonstrate better than words how little the age difference matters to them!) Each of the supporting characters is sharply drawn and excellently portrayed as well. The mix of pithy dialog and passionate excess makes this a delightful miniseries. As Russell Baker notes in his introduction, you may not be morally improved by viewing \"Reckless\" -- but you\\'ll have plenty of fun. (The sequel, a part of the DVD box set, provides a wild yet satisfying two-hour denouement. You won\\'t want to miss it if you\\'ve enjoyed what came before.)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lEJD0VPvxeJ",
        "colab_type": "code",
        "outputId": "5891d60c-b0c7-4cc4-a193-03ec506e1255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "class Corpus(Dataset):\n",
        "    def __init__(self, data, labels, max_seq_len):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        vectorizer = CountVectorizer(stop_words='english', min_df=0.01)\n",
        "        vectorizer.fit(data)\n",
        "        self.labels = labels\n",
        "        self.token2idx = vectorizer.vocabulary_\n",
        "        self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n",
        "        \n",
        "        self.tokenizer = vectorizer.build_analyzer() \n",
        "        \n",
        "        ## ADD YOUR CODE HERE\n",
        "        ## Encode review\n",
        "          \n",
        "        self.encoded_text = []\n",
        "        for review in data:\n",
        "            encoded_review = self.encodeReview(review)\n",
        "            self.encoded_text.append(encoded_review)\n",
        "        print(self.token2idx)\n",
        "        print(self.encoded_text[0])\n",
        "\n",
        "        # for review in data:\n",
        "        #     print(review)\n",
        "            \n",
        "    def encodeReview(self,review):\n",
        "        import string\n",
        "        encoded_text=[]\n",
        "        exclude = set(string.punctuation)\n",
        "        pad = self.token2idx['<PAD>']\n",
        "        sentence = review.lower().split()\n",
        "        for words in sentence:\n",
        "            word2 = ''.join(ch for ch in words if ch not in exclude) #remove the punctuat\n",
        "            if word2 in self.token2idx:\n",
        "                encoded_text.append(self.token2idx[word2])\n",
        "        num_pads = self.max_seq_len - len(encoded_text)\n",
        "        for i in range(num_pads):\n",
        "            encoded_text.append(self.token2idx['<PAD>'])\n",
        "        if len(encoded_text)>100:\n",
        "            del encoded_text[100:]\n",
        "        return encoded_text\n",
        "      \n",
        "    def __getitem__(self, i):\n",
        "        ## ADD YOUR CODE HERE\n",
        "        # return the encoded_text[i] and its label[i] \n",
        "        return self.encoded_text[i], self.labels[i]\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        ## ADD YOUR CODE HERE\n",
        "        # return the number of encoded_texts  \n",
        "        return len(self.encoded_text)\n",
        "                      \n",
        "        \n",
        "max_seq_len = 100        \n",
        "dataset = Corpus(train_data, train_labels, max_seq_len)   \n",
        "len(dataset.token2idx)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'mr': 907, 'movie': 904, 'far': 483, 'longer': 811, 'necessary': 926, 'aside': 88, 'terrific': 1355, 'sea': 1174, 'sequences': 1189, 'just': 731, 'did': 351, 'care': 190, 'characters': 212, 'character': 211, 'realized': 1098, 'early': 399, 'forgotten': 543, 'later': 763, 'time': 1373, 'really': 1099, 'problem': 1057, 'comes': 252, 'kid': 738, 'thinks': 1367, 'better': 135, 'shows': 1214, 'appears': 80, 'winning': 1494, 'finally': 518, 'past': 989, 'half': 607, 'way': 1475, 'point': 1029, 'tells': 1350, 'told': 1378, 'best': 134, 'magic': 831, 'turning': 1409, 'hour': 658, 'example': 445, 'action': 27, 'films': 516, 'boring': 153, 'worth': 1515, 'watching': 1473, 'complete': 264, 'waste': 1468, 'barely': 117, 've': 1442, 'times': 1374, 'acting': 26, 'don': 379, 'bother': 156, 'new': 931, 'jack': 713, 'city': 233, 'watch': 1470, 'york': 1530, 'real': 1093, 'deal': 327, 'dialogue': 350, 'makes': 837, 'film': 511, 'wondering': 1503, 'doing': 378, 'does': 375, 'play': 1019, 'seen': 1182, 'playing': 1022, 'died': 354, 'somewhat': 1245, 'br': 162, 'overall': 973, 'second': 1177, 'rate': 1086, 'trash': 1394, 'want': 1461, 'night': 935, 'copy': 286, 'script': 1173, 'thing': 1363, 'decent': 329, 'hand': 608, 'camera': 185, 'cinematography': 232, 'close': 241, 'making': 838, 'horrible': 654, 'quite': 1081, '10': 0, 'hate': 618, 'act': 24, 'gun': 602, 'shoot': 1205, 'like': 790, 'clich√£': 239, 'version': 1443, 'doesn': 376, 'minutes': 883, 'explain': 456, 'going': 584, 'single': 1223, 'guy': 603, 'brain': 163, 'william': 1490, 'villain': 1451, 'right': 1135, 'start': 1275, 'filled': 510, 'pointless': 1030, 'violence': 1452, 'people': 993, 'falling': 474, 'flying': 530, 'pretty': 1053, 'plot': 1026, 'big': 136, 'everybody': 441, 'dies': 355, 'deserve': 339, 'black': 142, 'actors': 30, 'annoying': 68, 'ugly': 1417, 'dead': 326, 'stay': 1282, 'away': 108, 'crap': 295, 'hours': 659, 'instead': 697, 'sense': 1185, 'humor': 663, 'cast': 195, 'write': 1519, 'songs': 1248, 'liked': 791, 'thought': 1369, 'movies': 905, 'came': 184, 'social': 1239, 'music': 912, 'turned': 1408, 'star': 1272, 'took': 1381, 'man': 840, 'accent': 19, 'saw': 1157, 'loved': 823, 'basic': 119, 'lives': 803, 'building': 177, 'hands': 609, 'quickly': 1080, 'setting': 1194, 'main': 832, 'location': 807, 'involved': 706, 'lame': 758, 'performances': 997, 'red': 1107, 'hot': 657, 'enjoyed': 426, 'opinion': 964, 'fell': 499, 'flat': 525, 'states': 1280, 'documentary': 374, 'stupid': 1303, 'twists': 1414, 'led': 778, 'got': 589, 'feeling': 496, 'set': 1192, 'confused': 270, 'things': 1364, 'enjoy': 424, 'scene': 1162, 'huge': 661, 'line': 795, 'yeah': 1526, 'ill': 670, 'feel': 495, 'aware': 107, 'stars': 1274, 'pictures': 1010, 'word': 1506, 'visual': 1454, 'high': 637, 'class': 234, 'tv': 1411, 'unfortunately': 1422, 'feature': 493, 'wanting': 1463, 'viewers': 1449, 'fail': 467, 'soft': 1241, 'non': 936, 'remake': 1116, 'title': 1376, 'lead': 770, 'original': 968, 'story': 1290, 'love': 822, 'italian': 712, 'moved': 902, 'century': 202, 'men': 868, 'german': 570, 'window': 1493, 'completely': 265, 'plays': 1023, 'attractive': 100, 'woman': 1497, 'falls': 475, 'puts': 1074, 'actress': 31, 'clearly': 237, 'material': 853, 'wide': 1487, 'fear': 492, 'reason': 1100, 'picture': 1009, 'basically': 120, 'stuff': 1301, 'wish': 1496, 'good': 586, 'parts': 985, 'come': 248, 'deserves': 341, 'funny': 556, 'happened': 611, 'hero': 633, 'director': 364, 'totally': 1384, 'make': 835, 'understand': 1420, 'excuse': 449, 'images': 672, 'english': 423, 'badly': 114, 'town': 1388, 'mention': 869, 'romantic': 1143, 'aspect': 92, 'oh': 955, 'sweet': 1332, 'miss': 884, 'horror': 655, 'connection': 272, 'child': 221, 'supposed': 1321, 'based': 118, 'true': 1401, 'result': 1125, 'father': 488, 'figure': 509, 'terribly': 1354, 'day': 324, 'manner': 844, 'blood': 145, 'girls': 577, 'course': 293, 'wouldn': 1517, 'fact': 466, 'usually': 1435, 'tears': 1344, 'eventually': 440, 'actually': 35, 'killing': 743, 'gets': 571, 'caught': 199, 'said': 1151, 'strange': 1293, 'dark': 320, 'slow': 1233, 'long': 810, 'fan': 479, 'japanese': 716, 'expected': 453, 'poor': 1034, 'little': 800, 'couldn': 289, 'stand': 1268, 'awful': 110, 'mess': 872, 'called': 182, 'pacing': 976, 'standard': 1269, 'noticed': 942, 'use': 1430, 'plots': 1027, 'different': 357, 'clear': 236, 'seriously': 1191, '12': 2, 'year': 1527, 'old': 958, 'book': 150, 'fans': 480, 'beat': 122, 'unless': 1425, 'need': 927, 'eyes': 463, 'girl': 575, 'cop': 284, 'flick': 527, '50': 7, 'talk': 1339, 'll': 805, 'video': 1446, 'walk': 1458, 'read': 1090, 'waiting': 1457, 'happen': 610, 'support': 1318, 'low': 828, 'end': 417, 'production': 1063, 'value': 1437, 'romance': 1142, 'sex': 1196, 'nudity': 944, 'heavy': 627, 'drama': 384, 'happens': 613, 'son': 1246, 'certain': 203, 'bright': 166, 'hair': 606, 'moves': 903, 'dance': 317, 'floor': 529, 'musical': 913, 'bit': 139, 'dancing': 318, 'gives': 579, 'constantly': 277, 'job': 720, 'stock': 1286, 'shots': 1209, 'filmed': 512, 'supporting': 1319, 'piece': 1011, 'hollywood': 647, 'literally': 799, 'mean': 856, 'pull': 1071, 'bad': 113, 'actor': 29, 'brilliant': 167, 'role': 1140, 'search': 1175, 'begins': 130, 'train': 1393, 'say': 1158, 'appear': 77, 'rented': 1121, 'wanted': 1462, 'think': 1365, 'disappointed': 366, 'outside': 971, 'box': 158, 'suspense': 1331, 'thriller': 1370, 'british': 170, 'novel': 943, 'lot': 819, 'twist': 1413, 'turns': 1410, 'leaves': 776, 'viewer': 1448, 'ending': 419, 'soon': 1249, 'started': 1276, 'note': 940, 'hit': 642, 'run': 1146, 'killed': 741, 'husband': 666, 'prove': 1066, 'idea': 667, 'wife': 1488, 'crime': 306, 'elements': 413, 'place': 1013, 'felt': 501, 'direction': 363, 'sort': 1251, 'opera': 963, 'wants': 1464, 'relationship': 1110, 'enjoyable': 425, 'lover': 825, 'beginning': 129, 'content': 279, 'kept': 734, 'leading': 771, 'event': 438, 'rating': 1088, 'short': 1207, 'expectations': 452, 'tell': 1348, 'house': 660, 'fully': 553, 'room': 1144, 'bar': 116, 'kind': 745, 'held': 628, 'mouth': 901, 'generally': 565, 'face': 464, 'girlfriend': 576, 'cinema': 230, 'audience': 101, 'attention': 99, 'look': 812, 'screen': 1171, 'looks': 815, 'summer': 1314, 'street': 1294, 'possible': 1042, 'included': 684, 'apparently': 75, 'state': 1279, 'offer': 952, 'male': 839, 'sexual': 1197, 'fantasy': 482, 'heart': 626, 'laughing': 767, 'loud': 821, 'tough': 1387, 'road': 1137, 'final': 517, 'disappointment': 368, 'incredible': 687, 'incredibly': 688, 'nice': 933, 'create': 297, 'young': 1531, 'learn': 773, 'know': 748, 'excellent': 446, 'performance': 996, 'usual': 1434, 'let': 782, 'able': 14, 'terrible': 1353, 'needed': 928, 'absolutely': 16, 'budget': 175, 'ed': 404, 'wood': 1504, 'art': 86, 'sets': 1193, 'worse': 1513, 'used': 1431, 'evil': 442, 'killer': 742, 'fight': 507, 'scenes': 1164, 'shot': 1208, 'expect': 451, '80s': 11, 'nasty': 920, 'talent': 1337, 'female': 502, 'taste': 1341, 'maybe': 855, 'standards': 1270, 'damn': 316, 'guess': 601, 'weren': 1482, 'available': 103, 'entertainment': 428, 'whatsoever': 1485, 'extremely': 461, 'cheap': 217, 'wonder': 1500, 'earth': 400, 'ended': 418, 'average': 104, 'post': 1044, 'theme': 1361, 'dangerous': 319, 'game': 558, 'running': 1147, 'certainly': 204, 'near': 924, 'fun': 554, 'voice': 1455, 'work': 1508, 'bland': 144, 'number': 945, 'entirely': 430, 'sequence': 1188, 'truly': 1402, 'points': 1031, 'manages': 843, 'kills': 744, 'mind': 880, 'murder': 909, 'wait': 1456, 'suppose': 1320, 'fair': 470, 'remains': 1115, 'watchable': 1471, 'presence': 1050, 'limited': 794, 'classic': 235, 'imagination': 673, 'worthy': 1516, 'ex': 443, 'friends': 552, 'cold': 244, 'lots': 820, 'fake': 472, 'costumes': 288, 'great': 595, 'guys': 604, 'moment': 892, 'earlier': 398, 'mystery': 915, 'sit': 1225, 'band': 115, 'opportunity': 965, 'victims': 1445, 'surprisingly': 1328, 'efforts': 411, 'jr': 728, 'kill': 740, 'sam': 1152, 'giving': 580, 'moral': 898, 'stories': 1289, 'including': 686, 'escape': 436, 'pace': 974, 'provides': 1069, 'women': 1498, 'speaking': 1259, 'tone': 1380, 'couple': 292, 'sexy': 1198, 'daughter': 322, 'today': 1377, 'person': 999, 'food': 537, 'friend': 551, 'sitting': 1226, 'easy': 402, 'simple': 1220, 'mother': 899, 'interested': 702, 'looking': 814, 'asks': 91, 'mom': 891, 'finish': 522, 'office': 954, 'slowly': 1234, 'white': 1486, 'slightly': 1232, 'suddenly': 1311, 'bed': 126, 'dream': 388, 'won': 1499, 'answer': 69, 'comedy': 251, 'actual': 34, 'obvious': 948, 'message': 873, 'justice': 732, 'police': 1032, 'forget': 542, 'ok': 956, 'family': 477, 'television': 1347, 'nearly': 925, 'david': 323, 'marriage': 846, 'rarely': 1085, 'happy': 614, 'human': 662, 'die': 353, 'slasher': 1230, 'garbage': 560, 'worst': 1514, '80': 10, 'following': 535, 'mentioned': 870, 'aspects': 93, 'dumb': 395, 'gore': 587, 'didn': 352, 'remember': 1117, 'wasn': 1467, 'likes': 793, 'jason': 717, 'situations': 1228, 'lack': 754, 'tension': 1351, 'works': 1511, 'simply': 1221, 'score': 1169, 'cheesy': 219, 'believe': 132, '20': 4, 'avoid': 105, 'thank': 1356, 'god': 582, 'tom': 1379, 'normal': 938, 'minor': 881, 'brief': 165, 'rich': 1131, 'spot': 1266, 'entertaining': 427, 'chemistry': 220, 'speak': 1258, 'week': 1479, 'quality': 1076, 'unnecessary': 1427, 'small': 1235, 'advice': 45, 'appreciate': 81, 'ones': 960, 'terms': 1352, 'honest': 649, 'feelings': 497, 'began': 127, 'particularly': 984, 'control': 281, 'actions': 28, 'moving': 906, 'went': 1481, 'understanding': 1421, 'change': 207, 'unlike': 1426, 'left': 780, 'driving': 391, 'car': 189, 'dog': 377, 'dying': 397, 'boyfriend': 160, 'ask': 89, 'baby': 111, 'hard': 615, 'working': 1510, 'crazy': 296, 'life': 786, 'added': 38, 'previous': 1054, 'acted': 25, 'open': 961, 'kick': 737, 'anti': 70, 'taking': 1335, 'middle': 877, 'age': 48, 'spent': 1262, 'convincing': 282, 'joy': 727, 'emotions': 416, 'sure': 1323, 'produced': 1060, 'students': 1299, 'decide': 330, 'begin': 128, 'getting': 572, 'mysterious': 914, 'climax': 240, 'trying': 1406, 'cute': 313, 'john': 722, 'favorite': 490, 'effects': 409, 'features': 494, 'sadly': 1150, 'pop': 1036, 'steve': 1284, 'victim': 1444, 'leave': 775, 'footage': 538, 'special': 1260, 'episode': 432, 'wild': 1489, 'joke': 723, 'worked': 1509, 'silent': 1217, 'minute': 882, 'makers': 836, 'sound': 1253, 'effect': 407, 'skip': 1229, 'rent': 1120, 'rated': 1087, 'physical': 1006, 'references': 1109, 'drug': 392, 'hell': 629, 'various': 1441, 'animals': 65, 'recently': 1103, 'form': 544, 'finding': 519, 'cover': 294, 'fantastic': 481, 'sounds': 1254, 'fat': 487, 'takes': 1334, 'ends': 420, 'starts': 1278, 'picked': 1008, 'isn': 709, 'shown': 1213, 'shame': 1199, 'effective': 408, 'recommend': 1104, 'leads': 772, 'stunning': 1302, 'looked': 813, 'space': 1257, 'laugh': 764, 'moments': 893, 'fit': 524, 'bizarre': 141, 'type': 1415, 'death': 328, 'scary': 1161, 'nature': 923, 'considered': 274, 'cult': 309, 'saving': 1156, 'grace': 592, 'lived': 802, 'plus': 1028, 'directed': 361, 'redeeming': 1108, 'level': 784, 'world': 1512, 'writers': 1521, 'dialog': 349, 'development': 348, 'names': 918, 'tale': 1336, 'exactly': 444, 'hear': 624, 'saying': 1159, 'entire': 429, 'filmmakers': 515, 'sat': 1153, 'hey': 635, 'probably': 1056, 'writer': 1520, 'board': 148, 'somebody': 1244, 'zero': 1533, 'fashion': 485, 'attempt': 97, 'photography': 1005, 'feels': 498, 'science': 1167, 'missing': 886, 'exciting': 448, 'cut': 312, 'rip': 1136, 'la': 753, 'cuts': 314, 'appearance': 78, 'park': 982, 'straight': 1292, 'field': 506, 'check': 218, 'rest': 1124, 'affair': 46, 'definitely': 335, 'strong': 1295, 'win': 1492, 'award': 106, 'locations': 808, 'apart': 73, 'seemingly': 1181, 'include': 683, 'jokes': 724, 'aren': 84, 'showing': 1212, 'fast': 486, 'forward': 545, 'years': 1528, 'area': 83, 'information': 692, 'ago': 50, 'case': 194, '30': 5, 'possibly': 1043, 'large': 761, 'live': 801, 'forever': 541, 'typical': 1416, 'natural': 921, 'supposedly': 1322, 'fine': 521, 'monster': 895, 'easily': 401, 'personal': 1000, 'list': 797, 'bunch': 178, 'kids': 739, 'trip': 1399, 'despite': 344, 'order': 967, 'leaving': 777, 'hold': 644, 'length': 781, 'obviously': 949, 'means': 858, 'release': 1112, 'talking': 1340, 'rock': 1139, 'local': 806, 'heads': 623, 'angry': 63, 'present': 1051, 'follow': 533, 'haven': 620, 'camp': 186, 'familiar': 476, 'touch': 1385, 'atmosphere': 95, 'business': 179, 'wrong': 1524, 'turn': 1407, 'known': 751, 'heard': 625, 'spoilers': 1265, 'matter': 854, 'purpose': 1073, 'crew': 305, 'similar': 1219, 'style': 1304, 'extra': 459, 'motion': 900, 'add': 37, 'total': 1383, 'says': 1160, 'shock': 1203, 'yes': 1529, 'paid': 977, 'lacks': 756, 'surprise': 1325, 'hadn': 605, 'intended': 700, 'question': 1077, 'days': 325, 'hasn': 617, 'loves': 826, 'thinking': 1366, 'interesting': 703, 'history': 641, 'channel': 210, 'according': 22, 'released': 1113, 'drive': 390, 'jim': 719, 'showed': 1211, 'children': 223, 'survive': 1329, 'older': 959, 'mid': 876, '70': 9, 'potential': 1045, 'cool': 283, 'gone': 585, 'ridiculous': 1134, 'phone': 1004, 'break': 164, 'telling': 1349, 'weak': 1477, 'dvd': 396, 'try': 1405, 'store': 1288, 'home': 648, 'impression': 681, 'jump': 730, 'gave': 561, 'thrown': 1372, 'watched': 1472, 'bought': 157, 'buy': 180, 'sheer': 1201, 'viewing': 1450, 'realize': 1097, 'editing': 406, 'coming': 254, 'disaster': 369, 'lacking': 755, 'experience': 455, 'green': 597, 'boy': 159, 'using': 1433, 'laughs': 768, 'dr': 383, 'brothers': 172, 'brother': 171, 'sick': 1215, 'body': 149, 'goes': 583, 'faces': 465, 'james': 714, 'tried': 1397, 'problems': 1058, 'subject': 1306, 'written': 1523, 'laughed': 766, 'grade': 593, 'portrayal': 1039, 'walking': 1459, 'pain': 978, 'truth': 1404, 'casting': 196, 'sorry': 1250, 'credit': 302, 'favor': 489, 'addition': 39, 'imagine': 674, 'school': 1165, 'club': 243, 'exist': 450, 'college': 246, 'attempts': 98, 'forced': 540, 'party': 986, 'adds': 40, 'attack': 96, 'imdb': 675, 'plain': 1015, 'studio': 1300, 'filming': 513, 'books': 151, 'boys': 161, 'situation': 1227, 'throw': 1371, 'frank': 546, 'knew': 747, 'credits': 303, 'computer': 267, 'failed': 468, 'london': 809, 'having': 621, 'accident': 21, 'hardly': 616, 'oscar': 970, 'writing': 1522, 'highly': 638, 'eye': 462, 'filmmaker': 514, 'accurate': 23, 'includes': 685, 'richard': 1132, 'younger': 1532, 'trouble': 1400, 'knowledge': 750, 'events': 439, 'american': 60, 'period': 998, 'starring': 1273, 'painful': 979, 'disturbing': 372, 'reading': 1091, 'positive': 1041, 'reviews': 1130, 'successful': 1309, 'series': 1190, 'happening': 612, 'spirit': 1263, 'concept': 268, 'teenage': 1346, 'powerful': 1047, 'force': 539, 'lines': 796, 'alive': 55, 'ghost': 573, 'lighting': 788, 'equally': 434, 'thanks': 1357, 'fails': 469, 'depth': 338, 'professional': 1064, 'peter': 1003, 'humour': 664, 'sees': 1183, 'helped': 631, 'save': 1154, 'money': 894, 'seeing': 1180, 'free': 548, 'chance': 206, 'track': 1389, 'missed': 885, 'places': 1014, 'wasted': 1469, 'adaptation': 36, 'comic': 253, 'drawn': 386, 'stick': 1285, 'premise': 1049, 'unique': 1423, 'intriguing': 704, 'comments': 257, 'comment': 255, 'bring': 168, 'clothes': 242, 'surprised': 1326, 'beautiful': 123, 'especially': 437, 'wearing': 1478, 'hope': 651, 'respect': 1122, 'help': 630, 'project': 1065, 'christmas': 227, 'desperate': 343, 'current': 311, 'sad': 1149, 'effort': 410, 'considering': 275, 'self': 1184, 'important': 678, 'tries': 1398, 'pass': 987, 'ultimately': 1418, 'sucks': 1310, 'deep': 333, 'treated': 1396, 'military': 878, 'hospital': 656, 'kevin': 735, 'stuck': 1297, 'sleep': 1231, 'commentary': 256, 'chase': 216, 'company': 259, 'honestly': 650, 'view': 1447, 'gem': 563, 'mark': 845, 'presented': 1052, 'unbelievable': 1419, 'nonsense': 937, 'narrative': 919, 'double': 381, 'western': 1484, 'mad': 830, 'predictable': 1048, 'singing': 1222, 'audiences': 102, 'fellow': 500, 'common': 258, 'comparison': 262, 'light': 787, 'famous': 478, 'career': 191, 'count': 290, 'mary': 849, 'difference': 356, 'secret': 1179, 'return': 1126, 'war': 1465, 'captured': 188, 'born': 154, 'particular': 983, 'carry': 192, 'taken': 1333, 'mistake': 887, 'direct': 360, 'emotion': 414, 'allow': 56, 'plane': 1017, 'station': 1281, 'popular': 1037, 'doctor': 373, 'screenplay': 1172, 'fiction': 505, 'lose': 817, 'producers': 1062, 'shouldn': 1210, 'lies': 785, 'jean': 718, 'news': 932, 'soundtrack': 1255, 'wooden': 1505, 'spoiler': 1264, 'reality': 1096, 'months': 896, 'americans': 61, 'success': 1308, 'meant': 859, 'battle': 121, 'contains': 278, 'ready': 1092, 'constant': 276, 'impossible': 679, 'smith': 1238, 'brings': 169, 'robert': 1138, 'hoping': 653, 'named': 917, 'knows': 752, 'mixed': 889, 'returns': 1127, 'late': 762, 'suspect': 1330, 'element': 412, 'perfectly': 995, 'okay': 957, 'rare': 1084, 'values': 1438, 'innocent': 693, 'confusing': 271, 'christopher': 228, 'flicks': 528, 'revenge': 1128, 'holes': 646, 'details': 345, 'color': 247, 'apparent': 74, 'difficult': 358, 'quick': 1079, 'amazing': 58, 'directing': 362, 'believable': 131, 'members': 864, 'runs': 1148, 'dull': 394, 'roles': 1141, 'compared': 261, 'epic': 431, 'sequel': 1187, 'sci': 1166, 'fi': 504, 'laughable': 765, 'design': 342, 'silly': 1218, 'learned': 774, 'trust': 1403, 'created': 298, 'helps': 632, 'academy': 18, 'super': 1315, 'brought': 173, 'followed': 534, 'wise': 1495, 'seven': 1195, 'poorly': 1035, 'animated': 66, 'planet': 1018, 'played': 1020, 'personality': 1001, 'ability': 13, 'king': 746, 'superior': 1317, 'clever': 238, 'review': 1129, 'spend': 1261, 'flaws': 526, 'greatest': 596, 'instance': 696, 'fall': 473, 'giant': 574, 'stage': 1267, 'artistic': 87, 'decided': 331, 'collection': 245, 'recent': 1102, 'creature': 301, 'mainly': 833, 'zombie': 1534, 'sister': 1224, 'ii': 669, 'adventure': 44, 'island': 708, 'group': 599, 'bits': 140, 'animation': 67, 'numerous': 947, '90': 12, 'theater': 1359, 'inside': 694, 'mood': 897, 'gang': 559, 'numbers': 946, 'shooting': 1206, 'ride': 1133, 'questions': 1078, 'smart': 1236, 'record': 1106, 'tired': 1375, 'talented': 1338, 'biggest': 138, 'opening': 962, 'lost': 818, 'wonderful': 1501, 'mediocre': 860, 'given': 578, 'stop': 1287, 'cgi': 205, 'modern': 890, 'ideas': 668, 'notice': 941, 'fresh': 550, 'knowing': 749, 'disappointing': 367, '15': 3, 'anybody': 71, '40': 6, 'conclusion': 269, 'loose': 816, 'scientist': 1168, 'childhood': 222, 'realistic': 1095, 'deliver': 336, 'joe': 721, 'managed': 842, 'immediately': 676, 'impact': 677, 'cartoon': 193, 'merely': 871, 'thoroughly': 1368, 'team': 1343, 'water': 1474, 'theatre': 1360, 'michael': 875, 'cinematic': 231, 'follows': 536, 'awesome': 109, 'explanation': 458, 'process': 1059, 'anymore': 72, 'head': 622, 'needs': 929, 'weird': 1480, 'consider': 273, 'folks': 532, 'cat': 197, 'season': 1176, 'image': 671, 'beauty': 125, 'mix': 888, 'industry': 691, 'starting': 1277, 'sent': 1186, 'ms': 908, 'met': 874, 'air': 53, 'intelligent': 699, 'sub': 1305, 'storyline': 1291, 'wow': 1518, 'torture': 1382, 'words': 1507, 'hated': 619, 'law': 769, 'extreme': 460, 'tragedy': 1390, 'teen': 1345, 'genre': 567, 'plan': 1016, 'nicely': 934, 'date': 321, 'prison': 1055, 'suggest': 1312, 'realism': 1094, 'likable': 789, 'bloody': 146, 'west': 1483, 'scott': 1170, 'lee': 779, 'martin': 848, 'blame': 143, 'share': 1200, 'stands': 1271, 'occasionally': 950, 'explained': 457, 'amusing': 62, 'responsible': 1123, 'impressive': 682, 'uses': 1432, 'reasons': 1101, 'major': 834, 'violent': 1453, 'future': 557, 'catch': 198, 'member': 863, 'heroes': 634, 'meaning': 857, 'directors': 365, 'indian': 690, 'ways': 1476, 'language': 760, 'finds': 520, 'acts': 33, 'key': 736, 'independent': 689, 'creative': 300, 'pathetic': 990, 'surely': 1324, 'dreadful': 387, 'decides': 332, 'student': 1298, 'lets': 783, 'bored': 152, 'french': 549, 'listen': 798, 'doubt': 382, 'agree': 51, 'pay': 992, 'charles': 213, 'south': 1256, 'machine': 829, 'cops': 285, 'keeps': 733, 'odd': 951, 'struggle': 1296, 'sight': 1216, 'pure': 1072, 'unknown': 1424, 'shocking': 1204, 'originally': 969, 'power': 1046, 'proves': 1067, 'van': 1440, 'alien': 54, 'involving': 707, 'hilarious': 639, 'continue': 280, 'parents': 980, 'fairly': 471, 'compelling': 263, 'soul': 1252, 'teacher': 1342, 'build': 176, 'public': 1070, 'warning': 1466, 'random': 1083, 'ups': 1429, 'religious': 1114, 'creepy': 304, 'paul': 991, 'normally': 939, 'complex': 266, 'gorgeous': 588, 'jane': 715, 'song': 1247, 'pieces': 1012, 'wrote': 1525, 'loving': 827, 'masterpiece': 851, 'fascinating': 484, 'charm': 214, 'inspired': 695, 'vampire': 1439, 'land': 759, 'ass': 94, 'match': 852, 'willing': 1491, 'disney': 371, 'paris': 981, 'door': 380, 'genius': 566, 'master': 850, 'appeal': 76, 'dad': 315, 'background': 112, 'asked': 90, 'personally': 1002, 'million': 879, 'solid': 1243, 'dramatic': 385, 'allowed': 57, 'engaging': 422, 'hopes': 652, 'holds': 645, 'admit': 41, 'perfect': 994, 'absolute': 15, 'lady': 757, 'naturally': 922, 'memorable': 865, 'changed': 208, 'changes': 209, 'meets': 862, 'afraid': 47, 'cause': 200, 'player': 1021, 'race': 1082, 'comedies': 250, 'america': 59, 'seconds': 1178, 'captain': 187, 'issue': 710, 'touching': 1386, 'cable': 181, 'married': 847, 'actresses': 32, 'naked': 916, 'country': 291, 'glad': 581, 'gay': 562, 'frankly': 547, 'issues': 711, 'saved': 1155, 'episodes': 433, 'negative': 930, 'wall': 1460, 'army': 85, 'murdered': 910, 'general': 564, 'pick': 1007, 'intelligence': 698, 'appeared': 79, 'focus': 531, 'adults': 43, 'gotten': 590, 'recommended': 1105, 'critics': 307, 'offers': 953, 'christian': 226, 'funniest': 555, 'plenty': 1025, 'opposite': 966, 'george': 569, 'murders': 911, 'meet': 861, 'hidden': 636, 'era': 435, 'portrayed': 1040, 'unusual': 1428, 'blue': 147, 'boss': 155, 'exception': 447, 'pleasure': 1024, 'scenery': 1163, 'paced': 975, 'hurt': 665, 'church': 229, 'rubbish': 1145, 'deeply': 334, 'dimensional': 359, 'grand': 594, 'growing': 600, 'expecting': 454, 'likely': 792, 'themes': 1362, 'cross': 308, 'utterly': 1436, 'delivers': 337, 'emotional': 415, 'subtle': 1307, 'suicide': 1313, 'drugs': 393, 'accept': 20, 'ahead': 52, 'putting': 1075, 'jones': 725, 'twice': 1412, 'charming': 215, 'calls': 183, 'eating': 403, 'fighting': 508, 'bigger': 137, 'absurd': 17, 'living': 804, 'step': 1283, 'intense': 701, 'treat': 1395, 'finished': 523, 'memory': 867, 'historical': 640, 'reminded': 1118, 'society': 1240, 'ground': 598, 'comedic': 249, 'hits': 643, 'adult': 42, 'developed': 347, 'tragic': 1391, 'journey': 726, 'ship': 1202, 'choice': 224, 'porn': 1038, 'producer': 1061, 'animal': 64, '100': 1, 'impressed': 680, 'discover': 370, 'culture': 310, 'energy': 421, 'creating': 299, 'manage': 841, 'wonderfully': 1502, 'passion': 988, 'approach': 82, 'introduced': 705, 'lovely': 824, 'beautifully': 124, 'detective': 346, 'compare': 260, 'government': 591, 'political': 1033, 'brutal': 174, 'agent': 49, 'trailer': 1392, 'dreams': 389, 'soldiers': 1242, 'ray': 1089, 'reminds': 1119, 'relationships': 1111, 'memories': 866, 'superb': 1316, 'central': 201, 'deserved': 340, 'genuine': 568, 'favourite': 491, 'edge': 405, 'core': 287, 'thats': 1358, '60': 8, 'chris': 225, 'surprising': 1327, 'outstanding': 972, 'judge': 729, 'smile': 1237, 'festival': 503, 'provide': 1068, 'ben': 133, '<PAD>': 1535}\n",
            "[907, 904, 483, 811, 926, 88, 1355, 1174, 1189, 731, 351, 190, 212, 211, 1098, 399, 543, 763, 1373, 351, 190, 211, 1099, 190, 1057, 252, 738, 1367, 135, 1214, 80, 1494, 518, 989, 607, 1475, 1029, 1350, 1378, 134, 831, 1409, 658, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKm81bnowDpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## convert each sequence and label to LongTensor and FloatTensor. \n",
        "def collate(batch):\n",
        "    inputs = torch.LongTensor([x[0] for x in batch])\n",
        "    target = torch.FloatTensor([x[1] for x in batch])\n",
        "    return inputs, target\n",
        "\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTGlqKG_4eF",
        "colab_type": "text"
      },
      "source": [
        "# Long-Short Term Memory Neural Network\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1dcjUz2wYWW43c3y3GHvsCjjSjceIV33a)\n",
        "\n",
        "\n",
        "Layer       | Operations           | Input Size          | Output Size\n",
        "------------| ---------------------|---------------------|---------------\n",
        "Layer 1     | Embedding            |  Sentence Size      | Embedding size\n",
        "Layer 2     | LSTM (NumOfLayer = 1)|  Embedding Size     | Hidden Size\n",
        "Layer 3     | Fully connected      |  Hidden Size        | 1 \n",
        "\n",
        "\n",
        "Embedding \n",
        "\n",
        "The nn.Embedding function takes at least two parameters: Vocabulary size and Embedding size \n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1xq_Pzde-M-SPWcGOafY-aN9JMGTT4i-N)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MLoSAv80ApB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    ## ADD YOUR CODE HERE\n",
        "\n",
        "    \n",
        "    def __init__(self, hidden_size, num_layers,num_classes, vocab_size,embedding_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.Embedding(vocab_size,embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_size, num_classes)     \n",
        "        self.weight_init()\n",
        "    \n",
        "    def weight_init(self):\n",
        "        ## Embedding layer\n",
        "        nn.init.xavier_uniform_(self.encoder.weight)\n",
        "        ## Fully connected layer\n",
        "        nn.init.xavier_uniform_(self.decoder.weight)\n",
        "\n",
        "      \n",
        "    def forward(self, inputs, hidden):\n",
        "        ## ADD YOUR CODE HERE\n",
        "        output = self.encoder(inputs)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.decoder(output[:, -1, :])\n",
        "        output = output.squeeze(1)\n",
        "        return output\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg9EoyrG0Erx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layer = 1\n",
        "hidden_size = max_seq_len \n",
        "\n",
        "## ADD YOUR CODE HERE\n",
        "num_classes = 1\n",
        "vocab_size = len(dataset.token2idx)\n",
        "embedding_size = 128\n",
        "model = LSTM(hidden_size, num_layer,num_classes,vocab_size,embedding_size)\n",
        "model = model.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "## binary_cross_entropy_with_logits (aka BCE with sigmoid) \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6D5vWI90R5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "0d2d20a3-fb0f-479a-ddf2-66d0ec7fc23c"
      },
      "source": [
        "num_epoch = 30\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epoch):\n",
        "    losses = []\n",
        "    total = 0\n",
        "    for i, (inputs, target) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        # num_layer x batch_size x hidden_size\n",
        "        h0 = torch.zeros(num_layer, inputs.size(0), hidden_size).to(device) \n",
        "        c0 = torch.zeros(num_layer, inputs.size(0), hidden_size).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        output = model(inputs, hidden)\n",
        "    \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "              \n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        total += 1\n",
        "    \n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    print(f'Epoch {epoch + 1}:\\tLoss: {epoch_loss:.3f}')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1:\tLoss: 0.683\n",
            "Epoch 2:\tLoss: 0.513\n",
            "Epoch 3:\tLoss: 0.318\n",
            "Epoch 4:\tLoss: 0.278\n",
            "Epoch 5:\tLoss: 0.251\n",
            "Epoch 6:\tLoss: 0.231\n",
            "Epoch 7:\tLoss: 0.213\n",
            "Epoch 8:\tLoss: 0.195\n",
            "Epoch 9:\tLoss: 0.176\n",
            "Epoch 10:\tLoss: 0.164\n",
            "Epoch 11:\tLoss: 0.151\n",
            "Epoch 12:\tLoss: 0.142\n",
            "Epoch 13:\tLoss: 0.154\n",
            "Epoch 14:\tLoss: 0.130\n",
            "Epoch 15:\tLoss: 0.126\n",
            "Epoch 16:\tLoss: 0.120\n",
            "Epoch 17:\tLoss: 0.108\n",
            "Epoch 18:\tLoss: 0.106\n",
            "Epoch 19:\tLoss: 0.104\n",
            "Epoch 20:\tLoss: 0.098\n",
            "Epoch 21:\tLoss: 0.095\n",
            "Epoch 22:\tLoss: 0.097\n",
            "Epoch 23:\tLoss: 0.089\n",
            "Epoch 24:\tLoss: 0.085\n",
            "Epoch 25:\tLoss: 0.085\n",
            "Epoch 26:\tLoss: 0.092\n",
            "Epoch 27:\tLoss: 0.083\n",
            "Epoch 28:\tLoss: 0.088\n",
            "Epoch 29:\tLoss: 0.084\n",
            "Epoch 30:\tLoss: 0.094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccVU_e204Rnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(text):\n",
        "    model.eval()                        \n",
        "    with torch.no_grad():               # do not save history\n",
        "        ## ADD YOUR CODE HERE\n",
        "        encoded_review = dataset.encodeReview(text)\n",
        "\n",
        "        inputs = torch.LongTensor(encoded_review)\n",
        "        inputs = inputs.to(device)\n",
        "        inputs = inputs.unsqueeze(0)\n",
        "        # num_layer x batch_size (now 1), hidden size\n",
        "        h0 = torch.zeros(num_layer, 1, hidden_size).to(device)\n",
        "        c0 = torch.zeros(num_layer, 1, hidden_size).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        prediction = model(inputs, hidden)\n",
        "        \n",
        "        return prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9MfASfx4U6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "897f36a5-ec81-47f8-9ed4-d0a489114d7f"
      },
      "source": [
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "threshold = 0 \n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    test_text = test_data[i]\n",
        "    prediction = predict_sentiment(test_text)\n",
        "    if (test_labels[i]):\n",
        "       if (prediction > threshold):\n",
        "          TP += 1\n",
        "       else: \n",
        "          FN += 1\n",
        "    else:\n",
        "      if (prediction > threshold):\n",
        "          FP += 1\n",
        "      else:\n",
        "          TN += 1\n",
        "   \n",
        "\n",
        "accuracy = (TP + TN) / (len(test_data))   \n",
        "\n",
        "print('Accuracy:',accuracy * 100)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.40279937791601\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}